import gorilla
import argparse
import os
import sys
from PIL import Image
import os.path as osp
import numpy as np
import random
import importlib
import json

import torch
import torchvision.transforms as transforms
import cv2

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.join(BASE_DIR, '..', 'Pose_Estimation_Model')
sys.path.append(os.path.join(ROOT_DIR, 'provider'))
sys.path.append(os.path.join(ROOT_DIR, 'utils'))
sys.path.append(os.path.join(ROOT_DIR, 'model'))
sys.path.append(os.path.join(BASE_DIR, 'model', 'pointnet2'))

# ===== SCORING CONFIGURATION =====
# Choose which scoring strategy to use for final detection ranking:
# 1: Use pose confidence only (original approach)
# 2: Use original segmentation score only (preserves segmentation ranking)  
# 3: Use combined score (weighted average of both)
SCORING_STRATEGY = 3

# If using combined scoring (strategy 3), set the weights:
SEG_SCORE_WEIGHT = 0.7    # Weight for original segmentation score
POSE_SCORE_WEIGHT = 0.3   # Weight for pose estimation confidence
# Note: SEG_SCORE_WEIGHT + POSE_SCORE_WEIGHT should equal 1.0
# ====================================


def debug_print(msg, debug_enabled=False):
    """Print debug messages only when debug is enabled"""
    if debug_enabled:
        print(msg)


def get_parser():
    parser = argparse.ArgumentParser(
        description="Pose Estimation")
    # pem
    parser.add_argument("--gpus",
                        type=str,
                        default="0",
                        help="path to pretrain model")
    parser.add_argument("--model",
                        type=str,
                        default="pose_estimation_model",
                        help="path to model file")
    parser.add_argument("--config",
                        type=str,
                        default="config/base.yaml",
                        help="path to config file, different config.yaml use different config")
    parser.add_argument("--iter",
                        type=int,
                        default=600000,
                        help="epoch num. for testing")
    parser.add_argument("--exp_id",
                        type=int,
                        default=0,
                        help="")
    
    # input
    parser.add_argument("--output_dir", nargs="?", help="Path to root directory of the output")
    parser.add_argument("--cad_path", nargs="?", help="Path to CAD(mm)")
    parser.add_argument("--rgb_path", nargs="?", help="Path to RGB image")
    parser.add_argument("--depth_path", nargs="?", help="Path to Depth image(mm)")
    parser.add_argument("--cam_path", nargs="?", help="Path to camera information")
    parser.add_argument("--seg_path", nargs="?", help="Path to segmentation information(generated by ISM)")
    parser.add_argument("--det_score_thresh", default=0.2, help="The score threshold of detection")
    
    # debug
    parser.add_argument("--debug", nargs="?", default="False", 
                       help="Enable debug output and debug image exports")
    args_cfg = parser.parse_args()

    return args_cfg

def init():
    args = get_parser()
    exp_name = args.model + '_' + \
        osp.splitext(args.config.split("/")[-1])[0] + '_id' + str(args.exp_id)
    log_dir = osp.join("log", exp_name)

    cfg = gorilla.Config.fromfile(args.config)
    cfg.exp_name = exp_name
    cfg.gpus     = args.gpus
    cfg.model_name = args.model
    cfg.log_dir  = log_dir
    cfg.test_iter = args.iter

    cfg.output_dir = args.output_dir
    cfg.cad_path = args.cad_path
    cfg.rgb_path = args.rgb_path
    cfg.depth_path = args.depth_path
    cfg.cam_path = args.cam_path
    cfg.seg_path = args.seg_path

    cfg.det_score_thresh = 0.37
    cfg.debug = True if args.debug=="True" else False  # Pass debug flag to config
    
    gorilla.utils.set_cuda_visible_devices(gpu_ids = cfg.gpus)

    return  cfg



from data_utils import (
    load_im,
    get_bbox,
    get_point_cloud_from_depth,
    get_resize_rgb_choose,
)
from draw_utils import draw_detections
import pycocotools.mask as cocomask
import trimesh

rgb_transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                    std=[0.229, 0.224, 0.225])])

def visualize(rgb, pred_rot, pred_trans, model_points, K, save_path):
    img = draw_detections(rgb, pred_rot, pred_trans, model_points, K, color=(255, 0, 0))
    img = Image.fromarray(np.uint8(img))
    img.save(save_path)
    prediction = Image.open(save_path)
    
    # concat side by side in PIL
    rgb = Image.fromarray(np.uint8(rgb))
    img = np.array(img)
    concat = Image.new('RGB', (img.shape[1] + prediction.size[0], img.shape[0]))
    concat.paste(rgb, (0, 0))
    concat.paste(prediction, (img.shape[1], 0))
    return concat


def _get_template(path, cfg, tem_index=1):
    rgb_path = os.path.join(path, 'rgb_'+str(tem_index)+'.png')
    mask_path = os.path.join(path, 'mask_'+str(tem_index)+'.png')
    xyz_path = os.path.join(path, 'xyz_'+str(tem_index)+'.npy')

    rgb = load_im(rgb_path).astype(np.uint8)
    xyz = np.load(xyz_path).astype(np.float32) / 1000.0  
    mask = load_im(mask_path).astype(np.uint8) == 255

    bbox = get_bbox(mask)
    y1, y2, x1, x2 = bbox
    mask = mask[y1:y2, x1:x2]

    rgb = rgb[:,:,::-1][y1:y2, x1:x2, :]
    if cfg.rgb_mask_flag:
        rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)

    rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
    rgb = rgb_transform(np.array(rgb))

    choose = (mask>0).astype(np.float32).flatten().nonzero()[0]
    if len(choose) <= cfg.n_sample_template_point:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point, replace=True)
    else:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point, replace=False)
    choose = choose[choose_idx]
    xyz = xyz[y1:y2, x1:x2, :].reshape((-1, 3))[choose, :]

    rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)
    return rgb, rgb_choose, xyz


def get_templates(path, cfg):
    n_template_view = cfg.n_template_view
    all_tem = []
    all_tem_choose = []
    all_tem_pts = []

    total_nView = 42
    for v in range(n_template_view):
        i = int(total_nView / n_template_view * v)
        tem, tem_choose, tem_pts = _get_template(path, cfg, i)
        all_tem.append(torch.FloatTensor(tem).unsqueeze(0).cuda())
        all_tem_choose.append(torch.IntTensor(tem_choose).long().unsqueeze(0).cuda())
        all_tem_pts.append(torch.FloatTensor(tem_pts).unsqueeze(0).cuda())
    return all_tem, all_tem_pts, all_tem_choose


def get_test_data(rgb_path, depth_path, cam_path, cad_path, seg_path, det_score_thresh, cfg, debug=False):
    # Input validation
    print("=> Validating input data...")
    if not os.path.exists(seg_path):
        raise FileNotFoundError(f"Segmentation file not found: {seg_path}")
    if not os.path.exists(depth_path):
        raise FileNotFoundError(f"Depth file not found: {depth_path}")
    if not os.path.exists(rgb_path):
        raise FileNotFoundError(f"RGB file not found: {rgb_path}")
    if not os.path.exists(cam_path):
        raise FileNotFoundError(f"Camera file not found: {cam_path}")
    if not os.path.exists(cad_path):
        raise FileNotFoundError(f"CAD file not found: {cad_path}")
    
    dets = []
    with open(seg_path) as f:
        dets_ = json.load(f) # keys: scene_id, image_id, category_id, bbox, score, segmentation
    
    print(f"=> Loaded {len(dets_)} detections from segmentation file")
    debug_print(f"=> DEBUG: Processing with debug flag enabled", debug)
    debug_print(f"=> DEBUG: Detection score threshold: {det_score_thresh}", debug)
    
    if len(dets_) == 0:
        raise ValueError("No detections found in segmentation file!")
    
    # Sort by score and take only the highest scoring detection
    dets_ = sorted(dets_, key=lambda d: d['score'], reverse=True)
    
    # Option 1: Process only the highest scoring detection
    if len(dets_) > 0:
        best_det = dets_[0]
        dets.append(best_det)
        print(f"Using highest scoring detection: {best_det['category_id']} {best_det['image_id']} {best_det['score']:.3f} {best_det['bbox']}")
    
    # Option 2: If you want to process multiple but prioritize the highest scoring one, uncomment below:
    # dets_ = dets_[:5]  # Take top 5
    # for det in dets_:
    #     dets.append(det)
    #     print(f"Get detection: {det['category_id']} {det['image_id']} {det['score']:.3f} {det['bbox']}")
    
    del dets_

    cam_info = json.load(open(cam_path))
    K = np.array(cam_info['cam_K']).reshape(3, 3)
    debug_print(f"=> Loaded camera intrinsics: {K}", debug)

    whole_image = load_im(rgb_path).astype(np.uint8)
    if len(whole_image.shape)==2:
        whole_image = np.concatenate([whole_image[:,:,None], whole_image[:,:,None], whole_image[:,:,None]], axis=2)
    debug_print(f"=> Loaded RGB image: {whole_image.shape}", debug)
    
    whole_depth = load_im(depth_path).astype(np.float32) * cam_info['depth_scale'] / 1000.0
    debug_print(f"=> Loaded depth image: {whole_depth.shape}, depth range: {np.min(whole_depth):.3f} - {np.max(whole_depth):.3f}m", debug)
    
    # Check if depth has valid values
    valid_depth_pixels = np.sum(whole_depth > 0)
    total_pixels = whole_depth.shape[0] * whole_depth.shape[1]
    debug_print(f"=> Valid depth pixels: {valid_depth_pixels}/{total_pixels} ({100*valid_depth_pixels/total_pixels:.1f}%)", debug)
    
    if valid_depth_pixels == 0:
        raise ValueError("Depth image contains no valid depth values!")
    
    whole_pts = get_point_cloud_from_depth(whole_depth, K)
    debug_print(f"=> Generated point cloud: {whole_pts.shape}", debug)

    mesh = trimesh.load_mesh(cad_path)
    model_points = mesh.sample(cfg.n_sample_model_point).astype(np.float32) / 1000.0
    radius = np.max(np.linalg.norm(model_points, axis=1))
    
    debug_print(f"=> CAD Model Analysis (for reference only - not used for filtering):", debug)
    debug_print(f"   Model points sampled: {len(model_points)}", debug)
    debug_print(f"   Model extent: X=[{np.min(model_points[:,0]):.3f}, {np.max(model_points[:,0]):.3f}], Y=[{np.min(model_points[:,1]):.3f}, {np.max(model_points[:,1]):.3f}], Z=[{np.min(model_points[:,2]):.3f}, {np.max(model_points[:,2]):.3f}]", debug)
    debug_print(f"   Model radius (max distance from origin): {radius:.3f}m", debug)
    debug_print(f"   NOTE: Using BOUNDING BOX approach - all valid points in 2D bbox region", debug)

    # Create visualization images to show which parts of depth are being used
    depth_usage_viz = np.zeros_like(whole_depth)  # Will show used depth regions
    segmentation_viz = np.zeros((whole_depth.shape[0], whole_depth.shape[1], 3), dtype=np.uint8)  # RGB visualization


    all_rgb = []
    all_cloud = []
    all_rgb_choose = []
    all_score = []
    all_dets = []
    
    print(f"Processing {len(dets)} detections...")
    
    for idx, inst in enumerate(dets):
        seg = inst['segmentation']
        score = inst['score']
        debug_print(f"Processing detection {idx} with score {score:.3f}", debug)

        # mask
        h,w = seg['size']
        try:
            rle = cocomask.frPyObjects(seg, h, w)
        except:
            rle = seg
        mask = cocomask.decode(rle)
        mask = np.logical_and(mask > 0, whole_depth > 0)
        
        debug_print(f"  Mask size: {mask.shape}, valid pixels: {np.sum(mask)}", debug)
        
        # DEBUG: Show depth values within the mask
        if np.sum(mask) > 0:
            mask_depth_values = whole_depth[mask]
            valid_depth_values = mask_depth_values[mask_depth_values > 0]
            debug_print(f"  DEBUG - Depth in mask: min={np.min(mask_depth_values):.3f}m, max={np.max(mask_depth_values):.3f}m, mean={np.mean(mask_depth_values):.3f}m", debug)
            debug_print(f"  DEBUG - Valid depth pixels in mask: {len(valid_depth_values)}/{len(mask_depth_values)} ({100*len(valid_depth_values)/len(mask_depth_values):.1f}%)", debug)
            if len(valid_depth_values) > 0:
                debug_print(f"  DEBUG - Valid depth range: {np.min(valid_depth_values):.3f}m to {np.max(valid_depth_values):.3f}m, mean={np.mean(valid_depth_values):.3f}m", debug)
        
        # Visualize the segmentation mask on the depth image
        color = np.array([255, 0, 0]) if idx == 0 else np.array([0, 255, 0])  # Red for first, green for others
        segmentation_viz[mask] = color
        
        # Show which depth pixels are being used for this detection
        depth_usage_viz[mask] = whole_depth[mask]
        
        if np.sum(mask) > 32:
            bbox = get_bbox(mask)
            y1, y2, x1, x2 = bbox
            debug_print(f"  Bbox: [{y1}, {y2}, {x1}, {x2}]", debug)
            
            # Draw bounding box on the segmentation visualization
            cv2.rectangle(segmentation_viz, (x1, y1), (x2, y2), (255, 255, 255), 2)
            # Add detection ID text
            cv2.putText(segmentation_viz, f'Det {idx}', (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            # DEBUG: Analyze depth within bounding box
            bbox_depth = whole_depth[y1:y2, x1:x2]
            bbox_mask = mask[y1:y2, x1:x2]  # Crop the mask to match bbox_depth dimensions
            
            debug_print(f"  DEBUG - Bbox depth analysis:", debug)
            debug_print(f"    Bbox size: {bbox_depth.shape}", debug)
            debug_print(f"    Bbox mask size: {bbox_mask.shape}", debug)
            debug_print(f"    Bbox depth range: {np.min(bbox_depth):.3f}m to {np.max(bbox_depth):.3f}m", debug)
            debug_print(f"    Bbox valid pixels: {np.sum(bbox_depth > 0)}/{bbox_depth.size} ({100*np.sum(bbox_depth > 0)/bbox_depth.size:.1f}%)", debug)
            
            # Show depth values specifically where the mask is True
            masked_depth_in_bbox = bbox_depth[bbox_mask]
            valid_masked_depth = masked_depth_in_bbox[masked_depth_in_bbox > 0]
            debug_print(f"    Masked depth in bbox: {len(valid_masked_depth)} valid pixels", debug)
            if len(valid_masked_depth) > 0:
                debug_print(f"    Masked depth range: {np.min(valid_masked_depth):.3f}m to {np.max(valid_masked_depth):.3f}m, mean={np.mean(valid_masked_depth):.3f}m", debug)
                debug_print(f"    Masked depth std: {np.std(valid_masked_depth):.3f}m", debug)
            
            # Save debug images only if debug mode is enabled
            if debug:
                # Save a debug image of this specific bbox region for inspection
                debug_bbox_depth = (bbox_depth / (np.max(bbox_depth) + 1e-8) * 255).astype(np.uint8)
                debug_bbox_with_mask = cv2.cvtColor(debug_bbox_depth, cv2.COLOR_GRAY2RGB)
                debug_bbox_with_mask[bbox_mask] = [255, 0, 0]  # Red overlay on mask
                
                debug_print(f"  Saving debug images for detection {idx}...", debug)
                
                # Use output_dir if available, otherwise create a debug folder in current directory
                try:
                    output_dir = cfg.output_dir
                except AttributeError:
                    output_dir = "debug_output"
                    debug_print(f"  Warning: cfg.output_dir not available, using '{output_dir}' instead", debug)
                
                os.makedirs(f"{output_dir}/debug_bbox", exist_ok=True)
                cv2.imwrite(f"{output_dir}/debug_bbox/detection_{idx}_bbox_depth.png", debug_bbox_depth)
                cv2.imwrite(f"{output_dir}/debug_bbox/detection_{idx}_bbox_with_mask.png", debug_bbox_with_mask)
            
            # Create depth image with bounding box visualization - only if debug enabled
            if debug:
                depth_with_bbox = whole_depth.copy()
                # Normalize depth for visualization
                depth_viz = (depth_with_bbox - np.min(depth_with_bbox)) / (np.max(depth_with_bbox) - np.min(depth_with_bbox) + 1e-8)
                depth_viz = (depth_viz * 255).astype(np.uint8)
                
                # Convert to RGB for colored bounding box
                depth_bbox_viz = cv2.cvtColor(depth_viz, cv2.COLOR_GRAY2RGB)
                
                # Draw bounding box in bright green
                cv2.rectangle(depth_bbox_viz, (x1, y1), (x2, y2), (0, 255, 0), 3)  # Green bbox with thickness 3
                
                # Add detection ID text
                cv2.putText(depth_bbox_viz, f'Detection {idx}', (x1, max(y1-10, 20)), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
                
                # Add bbox coordinates text
                bbox_text = f'Bbox: [{x1},{y1},{x2},{y2}]'
                cv2.putText(depth_bbox_viz, bbox_text, (x1, max(y1-35, 45)), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                # Add bounding box approach text
                approach_text = 'BOUNDING BOX APPROACH (All valid points in bbox)'
                cv2.putText(depth_bbox_viz, approach_text, (x1, max(y1-60, 70)), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)
                
                # Save the depth image with bounding box
                cv2.imwrite(f"{output_dir}/debug_bbox/detection_{idx}_depth_with_bbox.png", depth_bbox_viz)
                debug_print(f"    Saved depth with bbox: detection_{idx}_depth_with_bbox.png", debug)
        else:
            debug_print(f"  Skipping: insufficient mask pixels ({np.sum(mask)} <= 32)", debug)
            continue
            
        # pts - Use ALL VALID POINTS within the 2D bounding box region (not just segmentation mask)
        bbox_depth = whole_depth[y1:y2, x1:x2].flatten()
        bbox_pts = whole_pts.copy()[y1:y2, x1:x2, :].reshape(-1, 3)
        
        # Find all valid points in the bounding box (depth > 0)
        valid_depth_mask = bbox_depth > 0
        choose = valid_depth_mask.nonzero()[0]
        debug_print(f"  Choose points from bbox: {len(choose)} (all valid points)", debug)
        
        if len(choose) == 0:
            debug_print(f"  Skipping: no valid depth points in bounding box region", debug)
            continue

        # Get point cloud from all valid points in bounding box
        cloud = bbox_pts[choose, :]
        center = np.mean(cloud, axis=0) if len(cloud) > 0 else np.array([0, 0, 0])
        
        # DEBUG: Analyze point cloud using bounding box approach
        debug_print(f"  DEBUG - Bounding box point cloud analysis:", debug)
        debug_print(f"    Total valid points in bbox: {len(cloud)}", debug)
        if len(cloud) > 0:
            debug_print(f"    Cloud center: [{center[0]:.3f}, {center[1]:.3f}, {center[2]:.3f}]m", debug)
            debug_print(f"    Cloud extent: X=[{np.min(cloud[:,0]):.3f}, {np.max(cloud[:,0]):.3f}], Y=[{np.min(cloud[:,1]):.3f}, {np.max(cloud[:,1]):.3f}], Z=[{np.min(cloud[:,2]):.3f}, {np.max(cloud[:,2]):.3f}]", debug)
            debug_print(f"    Distance from origin: {np.linalg.norm(center):.3f}m", debug)
        
        # Use ALL valid points from the bounding box region
        debug_print(f"  Using all {len(cloud)} valid points from bounding box region", debug)
        
        # Check if we have enough points
        if len(cloud) < 4:
            debug_print(f"  Skipping: insufficient points in bbox ({len(cloud)} < 4)", debug)
            continue

        # Visualize the points that are actually used (all valid points in bbox)
        bbox_shape = whole_depth[y1:y2, x1:x2].shape
        points_used_mask = valid_depth_mask.reshape(bbox_shape)
        
        # Map back to full image coordinates
        full_points_mask = np.zeros_like(whole_depth, dtype=bool)
        full_points_mask[y1:y2, x1:x2] = points_used_mask
        
        # Mark these points in a different color for visualization
        point_color = np.array([255, 255, 0]) if idx == 0 else np.array([0, 255, 255])  # Yellow/Cyan
        segmentation_viz[full_points_mask] = point_color

        # Sample points for the model
        if len(choose) <= cfg.n_sample_observed_point:
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point, replace=True)
        else:
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point, replace=False)
        choose_sampled = choose[choose_idx]
        cloud = cloud[choose_idx]

        # rgb
        rgb = whole_image.copy()[y1:y2, x1:x2, :][:,:,::-1]
        if cfg.rgb_mask_flag:
            # For RGB masking, we can still use the original segmentation mask cropped to bbox
            mask_cropped_for_rgb = mask[y1:y2, x1:x2]
            rgb = rgb * (mask_cropped_for_rgb[:,:,None]>0).astype(np.uint8)
        rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
        rgb = rgb_transform(np.array(rgb))
        rgb_choose = get_resize_rgb_choose(choose_sampled, [y1, y2, x1, x2], cfg.img_size)

        all_rgb.append(torch.FloatTensor(rgb))
        all_cloud.append(torch.FloatTensor(cloud))
        all_rgb_choose.append(torch.IntTensor(rgb_choose).long())
        all_score.append(score)
        all_dets.append(inst)
        
        debug_print(f"  Successfully processed detection {idx}", debug)

    print(f"Successfully processed {len(all_cloud)} out of {len(dets)} detections")
    
    # Check if we have any valid detections
    if len(all_cloud) == 0:
        print("WARNING: No detections passed strict filtering criteria. Trying with relaxed criteria...")
        
        # Try again with more relaxed criteria
        for idx, inst in enumerate(dets):
            seg = inst['segmentation']
            score = inst['score']
            debug_print(f"Retry processing detection {idx} with relaxed criteria...", debug)

            # mask
            h,w = seg['size']
            try:
                rle = cocomask.frPyObjects(seg, h, w)
            except:
                rle = seg
            mask = cocomask.decode(rle)
            mask = np.logical_and(mask > 0, whole_depth > 0)
            
            # Also visualize relaxed criteria masks
            color = np.array([0, 0, 255])  # Blue for relaxed criteria
            segmentation_viz[mask] = color
            depth_usage_viz[mask] = whole_depth[mask]
            
            # Relaxed criterion 1: Accept masks with fewer pixels
            if np.sum(mask) > 16:  # Reduced from 32 to 16
                bbox = get_bbox(mask)
                y1, y2, x1, x2 = bbox
                
                # Draw bounding box for relaxed criteria
                cv2.rectangle(segmentation_viz, (x1, y1), (x2, y2), (255, 255, 0), 2)  # Yellow for relaxed
                cv2.putText(segmentation_viz, f'Relaxed {idx}', (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
            else:
                debug_print(f"  Still skipping: insufficient mask pixels ({np.sum(mask)} <= 16)", debug)
                continue
                
            # pts - Use ALL VALID POINTS within the 2D bounding box region (relaxed criteria)
            bbox_depth = whole_depth[y1:y2, x1:x2].flatten()
            bbox_pts = whole_pts.copy()[y1:y2, x1:x2, :].reshape(-1, 3)
            
            # Find all valid points in the bounding box (depth > 0)
            valid_depth_mask = bbox_depth > 0
            choose = valid_depth_mask.nonzero()[0]
            
            if len(choose) == 0:
                debug_print(f"  Still skipping: no valid depth points in bounding box region", debug)
                continue

            # Get point cloud from all valid points in bounding box
            cloud = bbox_pts[choose, :]
            center = np.mean(cloud, axis=0) if len(cloud) > 0 else np.array([0, 0, 0])
            
            # DEBUG: Relaxed criteria analysis
            debug_print(f"    RELAXED - Bounding box point cloud analysis:", debug)
            debug_print(f"      Valid points in bbox: {len(cloud)}", debug)
            if len(cloud) > 0:
                debug_print(f"      Cloud center: [{center[0]:.3f}, {center[1]:.3f}, {center[2]:.3f}]m", debug)
                debug_print(f"      Distance from origin: {np.linalg.norm(center):.3f}m", debug)
            
            # Relaxed criterion 2: Accept with fewer points
            if len(cloud) < 2:  # Reduced from 4 to 2
                debug_print(f"  Still skipping: insufficient points in bbox ({len(cloud)} < 2)", debug)
                continue

            # Visualize the points used in relaxed criteria (all valid points in bbox)
            bbox_shape = whole_depth[y1:y2, x1:x2].shape
            points_used_mask = valid_depth_mask.reshape(bbox_shape)
            
            # Map back to full image coordinates
            full_points_mask = np.zeros_like(whole_depth, dtype=bool)
            full_points_mask[y1:y2, x1:x2] = points_used_mask
            
            # Mark these points in white for relaxed criteria
            segmentation_viz[full_points_mask] = np.array([255, 255, 255])

            # Handle case where we have very few points
            if len(choose) <= cfg.n_sample_observed_point:
                choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point, replace=True)
            else:
                choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point, replace=False)
            choose_sampled = choose[choose_idx]
            cloud = cloud[choose_idx]

            # rgb
            rgb = whole_image.copy()[y1:y2, x1:x2, :][:,:,::-1]
            if cfg.rgb_mask_flag:
                # For RGB masking, use the original segmentation mask cropped to bbox
                mask_cropped_for_rgb = mask[y1:y2, x1:x2]
                rgb = rgb * (mask_cropped_for_rgb[:,:,None]>0).astype(np.uint8)
            rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
            rgb = rgb_transform(np.array(rgb))
            rgb_choose = get_resize_rgb_choose(choose_sampled, [y1, y2, x1, x2], cfg.img_size)

            all_rgb.append(torch.FloatTensor(rgb))
            all_cloud.append(torch.FloatTensor(cloud))
            all_rgb_choose.append(torch.IntTensor(rgb_choose).long())
            all_score.append(score)
            all_dets.append(inst)
            
            debug_print(f"  Successfully processed detection {idx} with relaxed criteria", debug)
            break  # Take the first one that works with relaxed criteria

    # Final check
    if len(all_cloud) == 0:
        print("\n" + "="*80)
        print("DIAGNOSIS: No valid detections found!")
        print("="*80)
        print("Using SEGMENTATION-ONLY approach (no radius filtering).")
        print("Here are the most likely causes and solutions:")
        print()
        print("1. SEGMENTATION QUALITY:")
        print("   - Check if masks cover the object properly")
        print("   - Verify mask has sufficient pixels (>32 for normal, >16 for relaxed)")
        print("   - Ensure segmentation mask aligns with valid depth pixels")
        print()
        print("2. DEPTH DATA ISSUES:")
        print("   - Check if depth_scale in camera.json is correct")
        print("   - Verify depth units (mm vs m)")
        print("   - Ensure depth image has valid values in segmented regions")
        print()
        print("3. CAMERA CALIBRATION:")
        print("   - Verify camera intrinsics are correct")
        print("   - Check if RGB and depth are properly aligned")
        print()
        print("4. POINT CLOUD GENERATION:")
        print("   - Ensure segmented regions contain enough valid 3D points")
        print("   - Check that depth values are reasonable in segmented areas")
        print()
        print("DEBUG FILES SAVED:")
        try:
            debug_output_dir = cfg.output_dir
        except AttributeError:
            debug_output_dir = "debug_output"
        print(f"   - {debug_output_dir}/debug_bbox/: Segmentation analysis")
        print("   - Check the depth values and mask overlays")
        print("="*80)
        
        raise ValueError(f"No valid detections found even with relaxed criteria! All {len(dets)} detections were filtered out.")

    print(f"Final result: Successfully processed {len(all_cloud)} detections")

    # Save depth usage visualization - only if debug enabled
    if debug:
        debug_print("=> Saving depth usage visualization...", debug)
    
    # Normalize depth for visualization
    depth_for_viz = whole_depth.copy()
    depth_for_viz = (depth_for_viz - np.min(depth_for_viz)) / (np.max(depth_for_viz) - np.min(depth_for_viz) + 1e-8)
    depth_for_viz = (depth_for_viz * 255).astype(np.uint8)
    
    # Create a 3-channel depth image for overlay
    depth_rgb = cv2.cvtColor(depth_for_viz, cv2.COLOR_GRAY2RGB)
    
    # Overlay segmentation masks on depth image
    overlay_alpha = 0.6
    depth_with_overlay = cv2.addWeighted(depth_rgb, 1-overlay_alpha, segmentation_viz, overlay_alpha, 0)
    
    # Save the visualizations - only if debug enabled
    try:
        viz_output_dir = cfg.output_dir
    except AttributeError:
        viz_output_dir = "debug_output"
        debug_print(f"Warning: cfg.output_dir not available, using '{viz_output_dir}' for visualizations", debug)
        
        os.makedirs(f"{viz_output_dir}/sam6d_results", exist_ok=True)
        
        # Save original depth image
        cv2.imwrite(os.path.join(f"{viz_output_dir}/sam6d_results", 'depth_original.png'), depth_for_viz)
        
        # Save depth with segmentation overlay
        cv2.imwrite(os.path.join(f"{viz_output_dir}/sam6d_results", 'depth_with_segmentation.png'), depth_with_overlay)
        
        # Save only the used depth regions
        depth_used_only = np.zeros_like(depth_for_viz)
        depth_used_mask = depth_usage_viz > 0
        if np.any(depth_used_mask):
            depth_used_normalized = (depth_usage_viz - np.min(depth_usage_viz[depth_used_mask])) / (np.max(depth_usage_viz[depth_used_mask]) - np.min(depth_usage_viz[depth_used_mask]) + 1e-8)
            depth_used_only[depth_used_mask] = (depth_used_normalized[depth_used_mask] * 255).astype(np.uint8)
        cv2.imwrite(os.path.join(f"{viz_output_dir}/sam6d_results", 'depth_used_regions.png'), depth_used_only)
        
        # Save segmentation masks only
        cv2.imwrite(os.path.join(f"{viz_output_dir}/sam6d_results", 'segmentation_masks.png'), segmentation_viz)
        
        debug_print(f"Depth visualization saved to {viz_output_dir}/sam6d_results/:", debug)
        debug_print(f"  - depth_original.png: Original depth image", debug)
        debug_print(f"  - depth_with_segmentation.png: Depth with segmentation overlay", debug)
        debug_print(f"  - depth_used_regions.png: Only the depth regions used for pose estimation", debug)
        debug_print(f"  - segmentation_masks.png: Segmentation masks visualization", debug)
        debug_print(f"Color coding:", debug)
        debug_print(f"  - Red: Primary detection mask", debug)
        debug_print(f"  - Yellow/Cyan: Points actually used for pose estimation (segmentation-only)", debug)
        debug_print(f"  - Blue: Relaxed criteria detection mask", debug)
        debug_print(f"  - White: Points used with relaxed criteria (segmentation-only)", debug)
        debug_print(f"  - White rectangles: Bounding boxes (normal criteria)", debug)
        debug_print(f"  - Yellow rectangles: Bounding boxes (relaxed criteria)", debug)

    ret_dict = {}
    
    ret_dict['pts'] = torch.stack(all_cloud).cuda()
    ret_dict['rgb'] = torch.stack(all_rgb).cuda()
    ret_dict['rgb_choose'] = torch.stack(all_rgb_choose).cuda()
    ret_dict['score'] = torch.FloatTensor(all_score).cuda()

    ninstance = ret_dict['pts'].size(0)
    ret_dict['model'] = torch.FloatTensor(model_points).unsqueeze(0).repeat(ninstance, 1, 1).cuda()
    ret_dict['K'] = torch.FloatTensor(K).unsqueeze(0).repeat(ninstance, 1, 1).cuda()
    # print the center of the detection
    # print(f"Center of the detection: {np.mean(ret_dict['pts'].cpu().numpy(), axis=1)}")
    return ret_dict, whole_image, whole_pts.reshape(-1, 3), model_points, all_dets



if __name__ == "__main__":
    cfg = init()

    random.seed(cfg.rd_seed)
    torch.manual_seed(cfg.rd_seed)

    # model
    print("=> creating model ...")
    MODEL = importlib.import_module(cfg.model_name)
    model = MODEL.Net(cfg.model)
    model = model.cuda()
    model.eval()
    checkpoint = os.path.join(os.path.dirname((os.path.abspath(__file__))), 'checkpoints', 'sam-6d-pem-base.pth')
    gorilla.solver.load_checkpoint(model=model, filename=checkpoint)

    print("=> extracting templates ...")
    try:
        tem_output_dir = cfg.output_dir
    except AttributeError:
        tem_output_dir = "debug_output"  
        debug_print(f"Warning: cfg.output_dir not available, using '{tem_output_dir}' for templates", cfg.debug)
    tem_path = os.path.join(tem_output_dir, 'templates')
    all_tem, all_tem_pts, all_tem_choose = get_templates(tem_path, cfg.test_dataset)
    with torch.no_grad():
        all_tem_pts, all_tem_feat = model.feature_extraction.get_obj_feats(all_tem, all_tem_pts, all_tem_choose)

    print("=> loading input data ...")
    input_data, img, whole_pts, model_points, detections = get_test_data(
        cfg.rgb_path, cfg.depth_path, cfg.cam_path, cfg.cad_path, cfg.seg_path, 
        cfg.det_score_thresh, cfg.test_dataset, cfg.debug
    )
    ninstance = input_data['pts'].size(0)
    
    print("=> running model ...")
    with torch.no_grad():
        input_data['dense_po'] = all_tem_pts.repeat(ninstance,1,1)
        input_data['dense_fo'] = all_tem_feat.repeat(ninstance,1,1)
        out = model(input_data)

    if 'pred_pose_score' in out.keys():
        pose_scores = out['pred_pose_score'] * out['score']
    else:
        pose_scores = out['score']
    pose_scores = pose_scores.detach().cpu().numpy()
    pred_rot = out['pred_R'].detach().cpu().numpy()
    pred_trans = out['pred_t'].detach().cpu().numpy() * 1000

    print("=> saving results ...")
    try:
        results_output_dir = cfg.output_dir
    except AttributeError:
        results_output_dir = "debug_output"
        debug_print(f"Warning: cfg.output_dir not available, using '{results_output_dir}' for results", cfg.debug)
    os.makedirs(f"{results_output_dir}/sam6d_results", exist_ok=True)
    for idx, det in enumerate(detections):
        # Preserve original segmentation score and combine with pose confidence
        original_seg_score = det['score']
        pose_confidence = float(pose_scores[idx])
        
        # Apply scoring strategy based on configuration
        if SCORING_STRATEGY == 1:
            # Strategy 1: Use pose confidence only (original approach)
            final_score = pose_confidence
        elif SCORING_STRATEGY == 2:
            # Strategy 2: Use original segmentation score only (preserves segmentation ranking)
            final_score = original_seg_score
        elif SCORING_STRATEGY == 3:
            # Strategy 3: Combine both scores (weighted average)
            final_score = SEG_SCORE_WEIGHT * original_seg_score + POSE_SCORE_WEIGHT * pose_confidence
        else:
            # Default to combined scoring
            final_score = 0.7 * original_seg_score + 0.3 * pose_confidence
        
        detections[idx]['score'] = final_score
        
        # Store both scores for reference
        detections[idx]['seg_score'] = original_seg_score
        detections[idx]['pose_score'] = pose_confidence
        
        detections[idx]['R'] = pred_rot[idx].tolist()
        detections[idx]['t'] = pred_trans[idx].tolist()
        
        debug_print(f"Detection {idx}: seg_score={original_seg_score:.3f}, pose_score={pose_confidence:.3f}, final_score={final_score:.3f} (strategy {SCORING_STRATEGY})", cfg.debug)

    with open(os.path.join(f"{results_output_dir}/sam6d_results", 'detection_pem.json'), "w") as f:
        json.dump(detections, f)

    print("=> visualizating ...")
    save_path = os.path.join(f"{results_output_dir}/sam6d_results", 'vis_pem.png')
    
    # Select the best detection based on the final score
    final_scores = [det['score'] for det in detections]
    best_idx = np.argmax(final_scores)
    print(f"Visualizing detection {best_idx} with final score {final_scores[best_idx]:.3f}")
    
    K = input_data['K'].detach().cpu().numpy()[[best_idx]]
    vis_img = visualize(img, pred_rot[[best_idx]], pred_trans[[best_idx]], model_points*1000, K, save_path)
    vis_img.save(save_path)
